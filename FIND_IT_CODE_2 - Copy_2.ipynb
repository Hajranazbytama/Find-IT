{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_features = pd.read_csv(r\"D:\\Lomba-Lomba\\Find IT UGM 2024\\data-analytics-competition-find-it-2024\\train_features.csv\")\n",
    "train_labels = pd.read_csv(r\"D:\\Lomba-Lomba\\Find IT UGM 2024\\data-analytics-competition-find-it-2024\\train_labels.csv\")\n",
    "test_features = pd.read_csv(r\"D:\\Lomba-Lomba\\Find IT UGM 2024\\data-analytics-competition-find-it-2024\\test_features.csv\")\n",
    "submission_format = pd.read_csv(r\"D:\\Lomba-Lomba\\Find IT UGM 2024\\data-analytics-competition-find-it-2024\\submission_format.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tahun_kelahiran</th>\n",
       "      <th>pendidikan</th>\n",
       "      <th>status_pernikahan</th>\n",
       "      <th>pendapatan</th>\n",
       "      <th>jumlah_anak_balita</th>\n",
       "      <th>jumlah_anak_remaja</th>\n",
       "      <th>terakhir_belanja</th>\n",
       "      <th>belanja_buah</th>\n",
       "      <th>belanja_daging</th>\n",
       "      <th>belanja_ikan</th>\n",
       "      <th>belanja_kue</th>\n",
       "      <th>pembelian_diskon</th>\n",
       "      <th>pembelian_web</th>\n",
       "      <th>pembelian_toko</th>\n",
       "      <th>keluhan</th>\n",
       "      <th>tanggal_menjadi_anggota</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979</td>\n",
       "      <td>Sarjana</td>\n",
       "      <td>Rencana Menikah</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50575.0</td>\n",
       "      <td>260967.0</td>\n",
       "      <td>50575.0</td>\n",
       "      <td>20230.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1950</td>\n",
       "      <td>Sarjana</td>\n",
       "      <td>Rencana Menikah</td>\n",
       "      <td>84063000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6069.0</td>\n",
       "      <td>44506.0</td>\n",
       "      <td>80920.0</td>\n",
       "      <td>20230.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1966</td>\n",
       "      <td>Sarjana</td>\n",
       "      <td>Menikah</td>\n",
       "      <td>127532564.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>117611.0</td>\n",
       "      <td>265460.0</td>\n",
       "      <td>96341.0</td>\n",
       "      <td>145573.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1961</td>\n",
       "      <td>Magister</td>\n",
       "      <td>Rencana Menikah</td>\n",
       "      <td>165579620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>206346.0</td>\n",
       "      <td>1613901.0</td>\n",
       "      <td>27725.0</td>\n",
       "      <td>125868.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970</td>\n",
       "      <td>Sarjana</td>\n",
       "      <td>Rencana Menikah</td>\n",
       "      <td>117703159.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>90563.0</td>\n",
       "      <td>311757.0</td>\n",
       "      <td>40358.0</td>\n",
       "      <td>33875.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tahun_kelahiran pendidikan status_pernikahan   pendapatan  \\\n",
       "0             1979    Sarjana   Rencana Menikah          NaN   \n",
       "1             1950    Sarjana   Rencana Menikah   84063000.0   \n",
       "2             1966    Sarjana           Menikah  127532564.0   \n",
       "3             1961   Magister   Rencana Menikah  165579620.0   \n",
       "4             1970    Sarjana   Rencana Menikah  117703159.0   \n",
       "\n",
       "   jumlah_anak_balita  jumlah_anak_remaja  terakhir_belanja  belanja_buah  \\\n",
       "0                 0.0                 1.0               NaN       50575.0   \n",
       "1                 NaN                 NaN              70.0        6069.0   \n",
       "2                 0.0                 0.0              45.0      117611.0   \n",
       "3                 0.0                 0.0              90.0      206346.0   \n",
       "4                 1.0                 1.0              78.0       90563.0   \n",
       "\n",
       "   belanja_daging  belanja_ikan  belanja_kue  pembelian_diskon  pembelian_web  \\\n",
       "0        260967.0       50575.0      20230.0               2.0            2.0   \n",
       "1         44506.0       80920.0      20230.0               9.0            6.0   \n",
       "2        265460.0       96341.0     145573.0               1.0            1.0   \n",
       "3       1613901.0       27725.0     125868.0               0.0            7.0   \n",
       "4        311757.0       40358.0      33875.0               7.0            6.0   \n",
       "\n",
       "   pembelian_toko  keluhan tanggal_menjadi_anggota  \n",
       "0             5.0      0.0              2014-05-05  \n",
       "1             4.0      0.0              2013-03-17  \n",
       "2             7.0      0.0                     NaN  \n",
       "3             8.0      0.0                     NaN  \n",
       "4             5.0      0.0                     NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in both training and test datasets\n",
    "missing_train = train_features.isnull().sum()\n",
    "missing_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_test = test_features.isnull().sum()\n",
    "missing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_train = train_features.duplicated().sum()\n",
    "duplicate_test = test_features.duplicated().sum()\n",
    "\n",
    "duplicate_train,\n",
    "#duplicate_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Variabel \" umur_anggota \" TRAIN\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "tahun_saat_ini_train = datetime.now().year\n",
    "train_features['usia'] = tahun_saat_ini_train - train_features['tahun_kelahiran']\n",
    "\n",
    "# Membuat Variabel \" umur_anggota \" Test\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "tahun_saat_ini_test = datetime.now().year\n",
    "test_features['usia'] = tahun_saat_ini_test - train_features['tahun_kelahiran']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jumlah anak\n",
    "\n",
    "train_features['jumlah_anak'] = train_features['jumlah_anak_balita']+train_features['jumlah_anak_remaja']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **HANDLING MISSING VALUE**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **imputasi biasa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3817, 25), (3818, 25))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Columns to be dropped\n",
    "columns_to_drop = ['tanggal_menjadi_anggota', 'tahun_kelahiran']\n",
    "\n",
    "# Categorical columns and numerical columns\n",
    "categorical_columns = ['pendidikan', 'status_pernikahan']\n",
    "numerical_columns = ['usia', 'pendapatan', 'jumlah_anak_balita', 'jumlah_anak_remaja', \n",
    "                     'terakhir_belanja', 'belanja_buah', 'belanja_daging', 'belanja_ikan', \n",
    "                     'belanja_kue', 'pembelian_diskon', 'pembelian_web', 'pembelian_toko', 'keluhan']\n",
    "\n",
    "# Define imputers\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Define OneHotEncoder for categorical data\n",
    "categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Define scaler for numerical data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Preprocessing pipeline for categorical data\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', categorical_imputer),\n",
    "    ('encoder', categorical_encoder)\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical data\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', numerical_imputer),\n",
    "    ('scaler', scaler)\n",
    "])\n",
    "\n",
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_pipeline, categorical_columns),\n",
    "        ('num', numerical_pipeline, numerical_columns)\n",
    "    ])\n",
    "\n",
    "# Fit preprocessor on train features and transform both train and test features\n",
    "train_features_preprocessed = preprocessor.fit_transform(train_features.drop(columns=columns_to_drop))\n",
    "test_features_preprocessed = preprocessor.transform(test_features.drop(columns=columns_to_drop))\n",
    "\n",
    "# Check the shape of the processed data\n",
    "train_features_preprocessed.shape, test_features_preprocessed.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **mice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3817, 25), (3818, 25))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Columns to be dropped\n",
    "columns_to_drop = ['tanggal_menjadi_anggota', 'tahun_kelahiran']\n",
    "\n",
    "# Categorical columns and numerical columns\n",
    "categorical_columns = ['pendidikan', 'status_pernikahan']\n",
    "numerical_columns = ['usia', 'pendapatan', 'jumlah_anak_balita', 'jumlah_anak_remaja', \n",
    "                     'terakhir_belanja', 'belanja_buah', 'belanja_daging', 'belanja_ikan', \n",
    "                     'belanja_kue', 'pembelian_diskon', 'pembelian_web', 'pembelian_toko', 'keluhan']\n",
    "\n",
    "# Define imputers\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "numerical_imputer = IterativeImputer()\n",
    "\n",
    "# Define OneHotEncoder for categorical data\n",
    "categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Define scaler for numerical data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Preprocessing pipeline for categorical data\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', categorical_imputer),\n",
    "    ('encoder', categorical_encoder)\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical data\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', numerical_imputer),\n",
    "    ('scaler', scaler)\n",
    "])\n",
    "\n",
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_pipeline, categorical_columns),\n",
    "        ('num', numerical_pipeline, numerical_columns)\n",
    "    ])\n",
    "\n",
    "# Fit preprocessor on train features and transform both train and test features\n",
    "train_features_preprocessed = preprocessor.fit_transform(train_features.drop(columns=columns_to_drop))\n",
    "test_features_preprocessed = preprocessor.transform(test_features.drop(columns=columns_to_drop))\n",
    "\n",
    "# Check the shape of the processed data\n",
    "train_features_preprocessed.shape, test_features_preprocessed.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **knn imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3817, 95), (3818, 95))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Columns to be dropped\n",
    "columns_to_drop = ['tanggal_menjadi_anggota', 'pendidikan']\n",
    "\n",
    "# Categorical columns and numerical columns\n",
    "categorical_columns = ['usia', 'status_pernikahan']\n",
    "numerical_columns = ['tahun_kelahiran', 'pendapatan', 'jumlah_anak_balita', 'jumlah_anak_remaja', \n",
    "                     'terakhir_belanja', 'belanja_buah', 'belanja_daging', 'belanja_ikan', \n",
    "                     'belanja_kue', 'pembelian_diskon', 'pembelian_web', 'pembelian_toko', 'keluhan']\n",
    "\n",
    "# Define imputers\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "numerical_imputer = KNNImputer()\n",
    "\n",
    "# Define OneHotEncoder for categorical data\n",
    "categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Define scaler for numerical data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Preprocessing pipeline for categorical data\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', categorical_imputer),\n",
    "    ('encoder', categorical_encoder)\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical data\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', numerical_imputer),\n",
    "    ('scaler', scaler)\n",
    "])\n",
    "\n",
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_pipeline, categorical_columns),\n",
    "        ('num', numerical_pipeline, numerical_columns)\n",
    "    ])\n",
    "\n",
    "# Fit preprocessor on train features and transform both train and test features\n",
    "train_features_preprocessed = preprocessor.fit_transform(train_features.drop(columns=columns_to_drop))\n",
    "test_features_preprocessed = preprocessor.transform(test_features.drop(columns=columns_to_drop))\n",
    "\n",
    "# Check the shape of the processed data\n",
    "train_features_preprocessed.shape, test_features_preprocessed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_preprocessed = pd.DataFrame(train_features_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "train_features_preprocessed.hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7035611940049276"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "scores = cross_val_score(rf_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_score = scores.mean()\n",
    "\n",
    "average_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tunning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "405 fits failed out of a total of 810.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "316 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "89 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\HAJRAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.7166816  0.72437802 0.72632243\n",
      " 0.68824876 0.70001532 0.70371688 0.64524061 0.65078875 0.65222704\n",
      " 0.68056617 0.68048032 0.68537235 0.66742678 0.68176753 0.67945902\n",
      " 0.6384376  0.6449378  0.64865364 0.62522265 0.62785487 0.63236968\n",
      " 0.62522265 0.62785487 0.63236968 0.60901389 0.61787951 0.61969198\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.60385484 0.60227615 0.60189431\n",
      " 0.58405703 0.58916946 0.59314206 0.5642793  0.56877941 0.56946538\n",
      " 0.58393532 0.58468919 0.58471533 0.58624506 0.58901971 0.58540221\n",
      " 0.56065497 0.56348513 0.56296494 0.55562781 0.55948485 0.56177041\n",
      " 0.55562781 0.55948485 0.56177041 0.54625866 0.55147766 0.55373508\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.71711273 0.71937778 0.72445901\n",
      " 0.68182025 0.69010854 0.69792871 0.64410164 0.64767328 0.65392942\n",
      " 0.68122985 0.68226152 0.6848494  0.67119457 0.68108889 0.68534932\n",
      " 0.63647066 0.64276487 0.64406616 0.62366889 0.62644706 0.62836427\n",
      " 0.62366889 0.62644706 0.62836427 0.60961253 0.61813957 0.61955793]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter terbaik: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Nilai F1 score terbaik: 0.7263224307081362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definisikan hyperparameter yang akan dituning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Inisialisasi GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, \n",
    "                           cv=5, scoring='f1_macro', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Lakukan pencarian parameter terbaik\n",
    "grid_search.fit(train_features_preprocessed, train_labels.squeeze())\n",
    "\n",
    "# Cetak parameter terbaik\n",
    "print(\"Parameter terbaik:\", grid_search.best_params_)\n",
    "\n",
    "# Cetak nilai F1 score terbaik\n",
    "print(\"Nilai F1 score terbaik:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7460585162518013"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Tunning Hyperparameter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "                                       n_estimators=1050,\n",
    "                                       max_depth= None,\n",
    "                                       min_samples_split=2,\n",
    "                                       min_samples_leaf = 1,\n",
    "                                       random_state=42)\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "scores = cross_val_score(rf_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_score = scores.mean()\n",
    "\n",
    "average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7477576715801167"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBOOST\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_classifier = XGBClassifier(n_estimators=1000, random_state=42)\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "xgb_scores = cross_val_score(xgb_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_xgb_score = np.mean(xgb_scores)\n",
    "\n",
    "average_xgb_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27017899796398553"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SVM Classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "svm_scores = cross_val_score(svm_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_svm_score = np.mean(svm_scores)\n",
    "\n",
    "average_svm_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5902689475998668"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADIEN BOOSTING\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "gb_scores = cross_val_score(gb_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_gb_score = np.mean(gb_scores)\n",
    "\n",
    "average_gb_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5297460195522833"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEURAL NETWORK\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the MLP Classifier\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=10000, random_state=42)\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "nn_scores = cross_val_score(nn_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_nn_score = np.mean(nn_scores)\n",
    "\n",
    "average_nn_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Logistic Regression Classifier\n",
    "logistic_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "logistic_scores = cross_val_score(logistic_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_logistic_score = np.mean(logistic_scores)\n",
    "\n",
    "average_logistic_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5724187033043248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Initialize the KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "knn_scores = cross_val_score(knn_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_knn_score = np.mean(knn_scores)\n",
    "\n",
    "average_knn_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5754391312000043"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DDT\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "dt_scores = cross_val_score(dt_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_dt_score = np.mean(dt_scores)\n",
    "\n",
    "average_dt_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13368360568249688"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAIVE BAYES\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes Classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Cross-validation to evaluate the model\n",
    "nb_scores = cross_val_score(nb_classifier, train_features_preprocessed, train_labels.squeeze(), cv=60, scoring='f1_macro')\n",
    "\n",
    "# Calculate average Macro F-Score across all folds\n",
    "average_nb_score = np.mean(nb_scores)\n",
    "\n",
    "average_nb_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>jumlah_promosi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5080</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  jumlah_promosi\n",
       "0  2241               0\n",
       "1  2274               0\n",
       "2  1107               1\n",
       "3  4478               0\n",
       "4  5080               4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the XGBoost model to the entire training data\n",
    "rf_classifier.fit(train_features_preprocessed, train_labels.squeeze(), )\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = rf_classifier.predict(test_features_preprocessed)\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_features['ID'],\n",
    "    'jumlah_promosi': test_predictions\n",
    "})\n",
    "\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "submission.to_csv(r'D:\\Lomba-Lomba\\Find IT UGM 2024\\data-analytics-competition-find-it-2024\\submission_predictions_11.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71ef6f62f9ca1c68c04779c8ddd79db32a478e7f229e5f9f1ee83cf7783afb78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
